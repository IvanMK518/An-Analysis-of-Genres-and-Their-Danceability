---
title: "An Analysis of Music Genres and Their Danceability"
author:
- name: Ivan Martinez-Kay
- name: Daniel Nickas
- name: Sandy Ge
affiliation: Emory University
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_notebook
---

# Music Dataset : 1950 to 2019

## Introduction

Music is a core human experience, a fundamental part of our evolution. Music is an outlet for people to express and communicate with others. Some produce it and more consume and share it. It’s part of life that defines many people’s identities and helps to emphasize moods, whether uplifting or depressive, energetic or calming. The purpose in utilizing music data helps to understand how people’s behaviors over time are reflected and impact the music we hear. Though not necessarily simply a matter of change over time, we can discover what aspects are critical characteristics of each specific genre of music and its era of release through musical metadata. “There are several works involving musical lyrics and topic modeling, one in specific and very common used is the Latent Dirichlet Allocation (LDA) [Blei 2003]” (Misael, Luan, et al., 2020). LDA is a probabilistic model used to find latent semantic topics in collections of texts such as lyrics. We can cite the LyricsRadar [Sasaki 2014] which is a system based on latent topics of lyrics generated by LDA that enables a user to visualize the music topics interactively. LDA can also be applied to identify sentiments from songs through the lyrics just as audio analysis can [Sharma 2011, Dakshina 2013]. Using methods of creating data out of music using LDA provides a baseline for how we organize and characterize music for algorithms, such as one that you may see on Spotify. LDA is the start of how we can use statistical data to show how playlists are chosen by users, what features influence musical choices, and in some cases predict hits. Using audio data scrapped using Echo Nest® API integrated engine with spotipy Python’s package. The spotipy API permits the user to search for specific genres, artists, songs, release dates, etc. To obtain the lyrics we used the Lyrics Genius® API as base URL for requesting data based on the song title and artist name. Of the numerous song characteristics shown by the publicly available spotify based dataset, we chose to use a select few for in depth analysis in our quest to find what genres of music are most associated with dancing and danceability. Diving deeper, we wanted to understand what characteristics of a song made one more or less danceable. 

The first step taken to begin the process of  filtering of data from this public music data set, was to install the Rstudio packages and libraries shown below:

```{r}
# Set CRAN mirror and install necessary packages
options(repos = c(CRAN = "https://cran.rstudio.com"))

install.packages("evaluate")
install.packages("renv")
install.packages("tidyverse")
install.packages("ggplot2")
install.packages("dplyr")
install.packages("magrittr")

# Snapshot, restore, and hydrate renv environment
renv::snapshot()
renv::restore()
renv::hydrate()

# Load necessary libraries
library(tidyverse)
library(readr)
library(ggplot2)
library(dplyr)
library(magrittr)
```

The code snippet below is used in an R Markdown document to control the global options for all code chunks in the document. When echo = True, the code is shown with the corresponding output which makes it easier for authors and readers to see what the code is producing.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

A preliminary step to filtering the data set is to test R and our music.csv file. This code snippet prints the first 6 observations, just to be sure we are working with the correct data set as R working directories may get crowded. 

```{r}
install.packages("tidyverse")
library(tidyverse)
```

```{r}
library(readr)
music <- read_csv("/Users/ivanmartinez-kay/QTM-302W/Data/music.csv") 
```
```{r}
head(music)
```

A few more preliminary steps in testing R and confirming the csv file are functioning are to play around with the csv file. 

```{r}
str(music)

print(names(music))

summary(music)
```

First, Using the "str" function helps us see the structure of the data set in that we have #31 columns with quantitative (numerical) and qualitative data such as a certain characteristic. The "print(names(music))" and "summary" functions just provide an organized view of the column titles and the sample characteristics (quartiles, median, and mode) of each column respectively. 

A data set with so much qualitative data may be initially difficult to process, but if the qualitative data is assigned numerical values, analyzing the data can be done with concrete methodology that can be replicated. In order to quantify and assign values to a seemingly qualitative characteristic such as lyrical theme, the creators of the public data set used:

EQUATION 1: 1. similarity = cos(Θ) = ϕi · ϕj /(ϕi ϕj)

This equation would compute a number between 0 and 1 that would effectively act as a piece of raw data representing a lyrical characteristic or theme of a song. According to the creators of the data set:

"We evaluated the cosine distance, Eqn. (1), of words per topic using a 100-dimension GloVe word embedding [Pennington 2014] vector representation. To visualize the best number of words to represent each topic, we select every word proportion per topic k represented by ϕk distribution by the most frequent term in that topic to the rarest term.For each topic, we have chosen the first 10 words of ϕK and computed the cosine distance for every pair of their embedding vectors and stored the mean result in a variable gk,10. We repeated the same methodology for the first 11 to 25 words of ϕk in each topic k calculating the mean cosine distance of the word embeddings. Based on those criteria, we select an appropriate number of words to represent each topic that maximizes the mean cosine similarity. Each topic received a label based on the main subject of the selected words. A stop word list was used to exclude common English words. The stop word list used is built-in Scikit-learn." (Misael, Luan, et al., 2020)

In our quest to find which songs and corresponding genres from the dataset were most danceable we considered a few song characteristics:

  * Acousticness: Presence of acoustic instruments
  * Danceability: how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat, strength, and overall regularity
  * Loudness: The average loudness in decibels (dB) across the entire track
  * Instrumentalness: A high value describes whether a track contains fewer vocals
  * Valence: High (low) values means that the track is more happy, euphoric (sad, angry)
  * Energy: Measures intensity and activity of music. Energetic tracks will be fast, loud and noisy
Each characteristic as mentioned would be assigned a value between 0-1 based on equation 1 (shown above).

## Analysis

The code snippet just below is selecting specific columns from our initial music.csv data set and assigning them to a new data frame called "data" that includes only these selected columns (consisting of data necessary for this coding and analysis).

```{r}
# Trimming dataset for specific variables
data <- music[ ,c("genre", "loudness", 
"danceability", "feelings", "instrumentalness", "acousticness",
"valence", "energy")]
```

In this small code chunk below, the median danceability by genre is found from the data set.

```{r}
median_danceability_by_genre <- data %>%
  group_by(genre) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

print(median_danceability_by_genre)
```

The first line is creating a data frame called "median_danceability_by_genre" from the "data" data frame and this pipe operator (%>%) allows the author to chain multiple functions together in a clear and readable way. The data frame data is passed to the next function in the chain. Then using the "group_by" function the data frame is grouped by the genre column. This means that subsequent operations will be performed separately for each genre. The 2nd to last line creates a new data frame containing summary statistics using "summmarise." In this case, it will create one row per genre. "median(danceability, na.rm = TRUE)" will calculates the median of the danceability column for each genre group from the data set. The na.rm = TRUE argument ensures that any NA (missing) values are removed before calculating the median. Finally, the new data frame is printed, showing a 2 column chart with 7 rows. Each row should show a genre in one column with a corresponding median danceability score in the other.

```{r}
# Bar chart of median danceability by genre
ggplot(median_danceability_by_genre, aes(x = genre, y = median_danceability)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Genre",
       x = "Genre",
       y = "Median Danceability")
```

From the above visualization, we notice that hiphop genre has the highest median danceability, closely followed by reggae, which is similar to what people would generally assume. The genre with the lowest danceability is rock, with the second lowest genre being blues. As blues is a rather slow genre, we are not surprised to see it ranked so low. However, rock, being such a rhythmic genre but ranking as the least danceable genre, is quite surprising to us. 

# Feeling

Next, we mainly want to visualize median danceability by different values of feeling. To do so, we first need to calculate the median danceability by feeling, which is accomplished by the first three lines of the following code, where the first line initiates a pipeline for the dataset, the second line grouping the data by the feeling columns, and the third line calculating the median of the danceability column for each group of feelings. As a sidenote, the na.rm = TRUE argument ensures that any NA (missing) values in the danceability column are ignored during the median calculation. Then, the fourth line prints the median_danceability_by_feeling dataframe, which contains the median danceability values for each feeling. The subsequent calculations will follow suit.

```{r}
library(dplyr)

# Calculate median danceability by feeling
median_danceability_by_feeling <- data %>%
  group_by(feelings) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

print(median_danceability_by_feeling)
```

With raw data, it is obviously very difficult to grasp what value or what range of values of feeling has the highest median danceability, which is why we separate the data into bins. 

The following code could be understood by grouping them into three different sets, as separated by the comments below. Feeling is a set of continuous variables, we cannot simply separate them into different columns. To this end, we set the number of bins to 20000 for condensing the data of musical feelings. Then, we start a pipeline for a new dataframe for visualization and create a 'feeling_bin' to contain the newly separated 17000 points of values for feelings.

```{r}
library(dplyr)
library(ggplot2)

# Group data by genre, calculate average feeling
average_feeling_by_genre <- data %>%
  group_by(genre) %>%
  summarise(average_feeling = mean(feelings, na.rm = TRUE)) %>%
  ungroup()

ggplot(average_feeling_by_genre, aes(x = genre, y = average_feeling)) +
  geom_bar(stat = "identity", fill = "blue") +
  geom_text(aes(label = sprintf("%.4f", average_feeling)), vjust = -0.5, color = "black") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Average Feeling by Genre",
       x = "Genre",
       y = "Average Feeling")
```
Here we can note that every single one of our genres fall into a range between 0.0 and 0.04, so for our purposes, we will filter the data and observe the behavior in this range.

```{r}
library(readr)
library(dplyr)
library(ggplot2)

# Bins to condense data of musical feel
num_bins <- 20000
data <- data %>%
  mutate(feeling_bin = cut(feelings, breaks = num_bins, labels = FALSE))

# Calculate median danceability by feel of music
median_danceability_by_feeling_bin <- data %>%
  group_by(feeling_bin) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

# Convert feeling_bin to factor for plotting
median_danceability_by_feeling_bin <- median_danceability_by_feeling_bin %>%
  mutate(feeling_bin = as.numeric(as.character(feeling_bin)))

# Filter data to the range 0.0 - 0.04
filtered_data <- median_danceability_by_feeling_bin %>%
  filter(feeling_bin >= 0 & feeling_bin <= 0.04 * num_bins)

# Define the cutoff values and convert them to bin labels
cutoff_values <- c(0.0372, 0.0306, 0.0176, 0.0291, 0.0308, 0.0303, 0.0299)
cutoff_bins <- cutoff_values * num_bins

ggplot(filtered_data, aes(x = feeling_bin / num_bins, y = median_danceability)) +
  geom_point(color = "blue") +
  scale_x_continuous(
    breaks = seq(0, 0.04, by = 0.01),
    labels = seq(0, 0.04, by = 0.01)
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Feel (0.0 - 0.04)",
       x = "Feel",
       y = "Median Danceability") +

  # Vertical lines at the cutoff values
  geom_vline(xintercept = cutoff_values, color = "red", linetype = "dashed")
```

We can observe that between 0.01 and 0.02 we find the most peaks in danceability. Hip hop lies in this range. Jazz cuts off just before 0.03 where there is a stiff dropoff, pointing to its lack of danceability. Blues also finds itself in a position just before a peak in danceability, but this is also the point where the drop is most notable (just before 0.04). Rock finds itself in an odd position, basically on the 0.03 line where there is a stiff dropoff and peak simultaneously; maybe we can use another metric for rock since it is the outlier. Country, reggae, and pop find themselves in a very similar region, but if you observe closely, reggae hits a peak just before pop and country drop off. So, rock remains our outlier for now. We will revisit this at the end.

# Instrumentality

```{r}
library(dplyr)
library(ggplot2)

# Group data by genre, calculate average feeling
average_instrumentalness_by_genre <- data %>%
  group_by(genre) %>%
  summarise(average_instrumentalness = mean(instrumentalness, na.rm = TRUE)) %>%
  ungroup()

ggplot(average_instrumentalness_by_genre, aes(x = genre, y = average_instrumentalness)) +
  geom_bar(stat = "identity", fill = "blue") +
  geom_text(aes(label = sprintf("%.4f", average_instrumentalness)), vjust = -0.5, color = "black") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Average Instrumentality by Genre",
       x = "Genre",
       y = "Average Instrumentality")
```

Moving onto the instrumentality, we again use similar principles, where we condense the coutinuous variable instrumentality into 20000 bins and peak within the range of 0.0 to 0.25, calculate the median danceability by instrumentality, and then visualize using a scatter plot. Jazz, blues, and rock have the highest instrumentality scores. To make a prediction, we think instrumentality will have an inverse relation to danceability. Let's confirm this with a scatterplot.

```{r}
library(readr)
library(dplyr)
library(ggplot2)

# Bins to condense data of musical feel
num_bins <- 200000
data <- data %>%
  mutate(instrumentalness_bin = cut(instrumentalness, breaks = num_bins, labels = FALSE))

# Calculate median danceability by feel of music
median_danceability_by_instrumentalness_bin <- data %>%
  group_by(instrumentalness_bin) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

# Convert feeling_bin to factor for plotting
median_danceability_by_instrumentalness_bin <- median_danceability_by_instrumentalness_bin %>%
  mutate(instrumentalness_bin = as.numeric(as.character(instrumentalness_bin)))

# Filter data to the range 0.0 - 0.25
filtered_data <- median_danceability_by_instrumentalness_bin %>%
  filter(instrumentalness_bin >= 0 & instrumentalness_bin <= 0.25 * num_bins)

# Cutoff values and convert them to bin labels
cutoff_values <- c(0.0945, 0.0183, 0.0154, 0.2336, 0.0461, 0.0502, 0.0928)
cutoff_bins <- cutoff_values * num_bins

ggplot(filtered_data, aes(x = instrumentalness_bin / num_bins, y = median_danceability)) +
  geom_point(color = "blue") +
  scale_x_continuous(
    breaks = seq(0, 0.25, by = 0.05),
    labels = seq(0, 0.25, by = 0.05)
  ) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Intrumentality (0.0 - 0.25)",
       x = "Instrumentality",
       y = "Median Danceability") +
  # Vertical lines at the cutoff values
  geom_vline(xintercept = cutoff_values, color = "red", linetype = "dashed")
```
We find that jazz finds its cutoff at a low point as suspected. What about blues and rock? Both pose as outliers in this case by encompassing a point that has both lows and peaks. that's one confirmation for blues, but rock still remains the single inconsistency. Our leader, hip hop, finds itself on an downward slope near the origin (not quite a peak but still higher than others), while it's follower country finds itself on a slight drop. One observation to note is that the danceability peaks near the origin, pointing to lower instrumentality being proportional to danceability. Reggae and op find themselves in the same scenario with reggae barely kissing a peak, and pop on a downward trend.

With this in mind, let's synthesize the data and see what we get.

# Synthesis

```{r}
# Bins for condensing data (feelings and instrumentalness variables)
num_bins_feelings <- 2
num_bins_instrumentalness <- 2

data <- data %>%
  mutate(feeling_bin = cut(feelings, breaks = num_bins_feelings),
         instrumentalness_bin = cut(instrumentalness, 
         breaks = num_bins_instrumentalness))

# Calculate median danceability by genre, feelings bin, and instrumentalness bin
median_danceability_by_genre_feeling_instrumentalness <- data %>%
  group_by(genre, feeling_bin, instrumentalness_bin) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

#Synthesis chart
ggplot(median_danceability_by_genre_feeling_instrumentalness, aes(x = interaction(feeling_bin, 
instrumentalness_bin), y = median_danceability, fill = genre)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Genre, Feeling, and Instrumentalness",
       x = "Feeling and Instrumentalness",
       y = "Median Danceability",
       fill = "Genre")
```

This graph synthesizes the information of the three previous bar charts. It once again demonstrates that even on a basis of feel and instumentality, reggae and hip hop still remain the most danceable before their respective cutoffs.

# Loudness

```{r}
library(dplyr)

# Calculate median danceability by decibel level
median_danceability_by_loudness <- data %>%
  group_by(loudness) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

print(median_danceability_by_loudness)
```
Finally, we want to explore the correlation between median danceability with decibel level for each track. To see a clearer trend, we did use bin to condense the variables into a fewer number of columns. This first line initializes a ggplot object using the median_danceability_by_loudness dataframe, setting loudness as the x-axis variable and median_danceability as the y-axis variable.

The resulting visualization has two peaks for median danceability, one around 0.35 Db and the other beyond 0.75 Db. There is also a very clear decrease in median danceability around 0.67 Db. Let's delve a bit deeper with a comparison of loudness by genre and a line graph.

```{r}
library(readr)
library(ggplot2)

# Bar chart of median danceability by decibel level
ggplot(median_danceability_by_loudness, aes(x = loudness, y = median_danceability)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Loudness",
       x = "Loudness (Db)",
       y = "Median Danceability")
```

```{r}
library(dplyr)
library(ggplot2)

# Group data by genre, calculate median loudness
median_loudness_by_genre <- data %>%
  group_by(genre) %>%
  summarise(median_loudness = median(loudness, na.rm = TRUE)) %>%
  ungroup()

ggplot(median_loudness_by_genre, aes(x = genre, y = median_loudness)) +
  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = 1) +
  geom_text(aes(label = sprintf("%.4f", median_loudness)), vjust = -0.5, color = "black") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Loudness by Genre",
       x = "Genre",
       y = "Median Loudness")
```

What we find once again is that hip hop, reggae, and rock are the three loudest. The trio all sit beyond the .7 db range. Hip hop and reggae make sense, but once again, rock proves itself to be the outlier; loudness seems to be directly proportional to danceability otherwise. All of our other values are much too large to hit the initial peak around .35 db.

```{r}
library(readr)
library(ggplot2)

# Line graph of median danceability by decibel level
ggplot(median_danceability_by_loudness, aes(x = loudness, y = median_danceability)) +
  geom_line(color = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Loudness",
       x = "Loudness (Db)",
       y = "Median Danceability")
```

Taking a look at the line graph, we indeed find that there are many peaks in danceability beyond the .7 db mark. We also find that the highest peak in danceability lies within this range, further pointing to our predictions.

# Acousticness

Moving on to acousticness. We wanted to see how this tied into danceability.

Let's observe it's properties first.

```{r}
library(dplyr)

# Calculate median danceability by acousticness
median_danceability_by_acousticness <- data %>%
  group_by(acousticness) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

print(median_danceability_by_acousticness)
```

```{r}
library(readr)
library(ggplot2)

# Line graph of median danceability by acousticness
ggplot(median_danceability_by_acousticness, aes(x = acousticness, y = median_danceability)) +
  geom_line(color = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Acousticness",
       x = "Acousticness",
       y = "Median Danceability")
```

Seemingly, the lower the acousticness, the higher the danceability. The highest peaks sit closest to the origin, slowly creeping down as we move further away. We also see an all time low near our highest value of acousticness. What could this mean? 

Let's take a look at acousticness of genres.

```{r}
library(dplyr)
library(ggplot2)

# Group data by genre, calculate median acousticness
median_acousticness_by_genre <- data %>%
  group_by(genre) %>%
  summarise(median_acousticness = median(acousticness, na.rm = TRUE)) %>%
  ungroup()

ggplot(median_acousticness_by_genre, aes(x = genre, y = median_acousticness)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Acousticness by Genre",
       x = "Genre",
       y = "Median Acousticness")
```

Reggae, hip hop, and rock are staggeringly low! Which confirms our suspicion of peak danceability being at lower acoustic levels. Jazz, our lowest danceability thus far has the highest level of acousticness. Our other low scorers also have decently high acoustic scores. However, we still have a problem, rock remains our sole outlier again. Let's delve a bit deeper.

# Valence

```{r}
library(dplyr)
library(ggplot2)

# Group data by genre, calculate median valence
median_valence_by_genre <- data %>%
  group_by(genre) %>%
  summarise(median_valence = median(valence, na.rm = TRUE)) %>%
  ungroup()

ggplot(median_valence_by_genre, aes(x = genre, y = median_valence)) +
  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = 1) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Valence by Genre",
       x = "Genre",
       y = "Median Valence")
```

Rock has a pitifully low score in terms of valence while our two leaders have insanely high scores. Let's see if we can finally prove that rock isn't danceable. 

A comparison of valence and danceability could prove to be what we need.

```{r}
library(dplyr)

# Calculate median danceability by valence
median_danceability_by_valence <- data %>%
  group_by(valence) %>%
  summarise(median_danceability = median(valence, na.rm = TRUE))

print(median_danceability_by_valence)

library(readr)
library(ggplot2)

# Line graph of median danceability by decibel level
ggplot(median_danceability_by_valence, aes(x = valence, y = median_danceability)) +
  geom_line(color = "black") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Valence",
       x = "Valence",
       y = "Median Danceability")
```

Valence proves to have a linear and directly proportional relationship with danceability. It is absolute that if valence is high, danceability is high. This is why rock lacks in danceability. It lacks the euphoric highs that both reggae and hip hop are adept at producing.

Which brings us to our final variable, energy.

# Energy

```{r}
# Group data by genre, calculate median feeling
median_energy_by_genre <- data %>%
  group_by(genre) %>%
  summarise(median_energy = median(energy, na.rm = TRUE)) %>%
  ungroup()

# Create a bar chart to visualize median feeling by genre
ggplot(median_energy_by_genre, aes(x = genre, y = median_energy)) +
  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = 1) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Energy by Genre",
       x = "Genre",
       y = "Median Energy")
```
Interestingly enough, rock is the highest energy genre, further pointing it out as the outlier from earlier. Hip hop once again outscores the rest (besides rock), with reggae seemingly lagging behind, but still maintaining a respectable energy score. 

```{r}
library(dplyr)
library(ggplot2)

# Assuming your data frame is named 'data'

# Group data by genre, calculate median feeling
median_danceability_by_energy <- data %>%
  group_by(energy) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE)) %>%
  ungroup()

# Create a bar chart to visualize median feeling by genre
ggplot(median_danceability_by_energy, aes(x = energy, y = median_danceability)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Energy",
       x = "Energy",
       y = "Median Danceability")
```
However, If we take a closer look at which energy values correlate to a higher danceability we can note a few observations.
```{r}
# Create a bar chart to visualize median feeling by genre
ggplot(median_danceability_by_energy, aes(x = energy, y = median_danceability)) +
  geom_line(color = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Energy",
       x = "Energy",
       y = "Median Danceability")
```

Median danceability has high fluctuations at lower energy values, while hitting its peak. However it stabilizes at 0.25 to .75 before starkly dropping off beyond. Rock happens to have a energy value far beyond .7 showing exactly why there is a danceability dropoff. Hip hop hits the sweet spot right before the dropoff, and reggae finds itself amidst the peak values where energy is proportional to danceability. We have found statistics that show our outlier isn't as troublesome as we thought.

## Conclusion

Overall, in this report we focus on the danceability of different genres, aiming to find the relationship between five other variables: feeling, instrumentality, loudness, valence, and energy. Although rock remained the outlier through our analysis of feeling, instrumentality, and loudness, valence and energy gave a basis to our initial hypothesis. What we have found is that instrumentality and acousticness have an inverse relationship with danceability. Our lowest scorers tended to have the highest scores in these categories. On the other side of the spectrum, valency had a directly proportional (linear) relationship with danceability. Our leaders had the highest scores in this category. Loudness though not linear provided a clear elimination of our outlier: rock. Feeling yielded the most inconsistent results with some slight contradictions.

Danceability is an exciting song characteristic, but it is by no means the only one that can be analyzed. Additionally, “data preparation” and analyses similar to this code notebook breakdown function as a baseline for song recommendation algorthims on streaming platforms. Taking playlist data and finding matches in songs (and artists) with similar characeristics is a large part of manufacturing popularity surrounding new music.

To conclude, we have found hip hop and reggae to be the most danceable for the following reasons: low acousticness, low instrumentality, high loudness, high valency, and mild energy. We believe that this points to another idea that lyrical and groovy music is more danceable. With less melodic complexities, it is easier to reap the benefits of danceability.


## Works cited:

Shahane, S. (2019). Music Dataset: 1950 to 2019 [Data set]. Kaggle. https://www.kaggle.com/datasets/saurabhshahane/music-dataset-1950-to-2019/data

Misael, L., Forster, C., Fontelles, E., Sampaio, V., & França, M. (2020, October). Temporal analysis and visualisation of music. In Anais do XVII Encontro Nacional de Inteligência Artificial e Computacional (pp. 507-518). SBC.
