# QTM-302W

Components of Analysis (from canvas, basically the rubric for the assignment)

Introducing the dataset (including its source, scope, and any ethical considerations) framed by the purpose of the exploratory analysis, anticipating any subsetting or re-ordering of the data for your interests
Reading the data into R in a tabular format, identifying, subsetting, and renaming the variables for your use
Forming the core data frame that you'll be working from by reshaping and summarizing the data, storing new data frames in memory where appropriate
Cleaning the data according to your purpose for the dataset
Plotting distributions of your data
Calculating basic statistical metrics like central tendency measures
Some light correlational analysis/visualization of key variables.
A look forward to what further questions the analysis suggests and what it enables
Music Dataset : 1950 to 2019



Introduction


Music is a core human experience, a fundamental part of our evolution. Music is an outlet for people to express and communicate with others. Some produce it and more consume and share it. It’s part of life that defines many people’s identities and helps to emphasize moods, whether uplifting or depressive, energetic or calming. The purpose in utilizing music data helps to understand how people’s behaviors over time are reflected and impact the music we hear. Though not necessarily simply a matter of change over time, we can discover what aspects are critical characteristics of each specific genre of music and its era of release through musical metadata. Citing https://sol.sbc.org.br/index.php/eniac/article/view/12155/12020, “There are several works involving musical lyrics and topic modeling, one in specific and very common used is the Latent Dirichlet Allocation (LDA) [Blei 2003].” LDA is a probabilistic model used to find latent semantic topics in collections of texts such as lyrics. We can cite the LyricsRadar [Sasaki 2014] which is a system based on latent topics of lyrics generated by LDA that enables a user to visualize the music topics interactively. LDA can also be applied to identify sentiments from songs through the lyrics just as audio analysis can [Sharma 2011, Dakshina 2013]. Using methods of creating data out of music using LDA provides a baseline for how we organize and characterize music for algorithms, such as one that you may see on Spotify. LDA is the start of how we can use statistical data to show how playlists are chosen by users, what features influence musical choices, and in some cases predict hits. Using audio data scrapped using Echo Nest® API integrated engine with spotipy Python’s package. The spotipy API permits the user to search for specific genres, artists, songs, release dates, etc. To obtain the lyrics we used the Lyrics Genius® API as base URL for requesting data based on the song title and artist name. Using LDA and the spotipy API, the public dataset utilized here contains data from music produced from 1950 to 2019, fetched from Spotify. We obtained the dataset from Kaggle (DOI: 10.17632/3t9vbwxgr5.3), where the description said, "This dataset provides a list of lyrics from 1950 to 2019 describing music metadata in the form of these selected features:


Acousticness
: Presence of acoustic instruments;


Danceability
: how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity;


Loudness
The average loudness in decibels (dB) across the entire track;


Instrumentalness
: A high value describes whether a track contains fewer vocals;


Valence
: High (low) values means that the track is more happy, euphoric (sad,angry);


Energy
: Measures intensity and activity of music. Energetic tracks will be fast,loud and noisy


In order to quantify and assign values to a seemingly qualitative characteristic such as lyrical theme, the creators of the public dataset used this equation:

similarity = cos(Θ) = ϕi · ϕj /(ϕi ϕj)

This equation would compute a number between 0 and 1 that would effectively act as a piece of raw data representing a lyrical characteristic or theme of a song. According to the creators of the dataset:

We evaluated the cosine distance, Eqn. (1), of words per topic using a 100-dimension GloVe word embedding [Pennington 2014] vector representation. To visualize the best number of words to represent each topic, we select every word proportion per topic k represented by ϕk distribution by the most frequent term in that topic to the rarest term.
For each topic, we have chosen the first 10 words of ϕK and computed the cosine distance for every pair of their embedding vectors and stored the mean result in a variable gk,10. We repeated the same methodology for the first 11 to 25 words of ϕk in each topic k calculating the mean cosine distance of the word embeddings. Based on those criteria, we select an appropriate number of words to represent each topic that maximizes the mean cosine similarity. Each topic received a label based on the main subject of the selected words. A stop word list was used to exclude common English words. The stop word list used is built-in Scikit-learn. Here we display some selected words of the principal topics and the number in parenthesis represents the mean cosine similarity of the topic words: 

• dating (0.442): ‘baby’ ‘babe’ ‘loving’ ‘lady’ ‘tell’ ‘darling’ ‘crazy’ ‘cause’ ‘leave’ ‘say’ ‘right’ ‘girl’ ‘need’ ‘treat’ ‘bring’ 
• violence (0.492): ‘head’ ‘stand’ ‘black’ ‘come’ ‘dead’ ‘fight’ ‘kill’ ‘blood’ ‘hand’ ‘cold’ ‘face’ • world/life (0.631): ‘life’ ‘live’ ‘world’ ‘change’ ‘time’ ‘believe’ ‘days’ ‘remember’ ‘dream’ ‘things’ ‘come’ 
• night/time (0.576): ‘time’ ‘right’ ‘night’ ‘long’ ‘tonight’ ‘mind’ ‘wait’ ‘stay’ ‘ready’ ‘line’ 
• shake the audience (0.434): ‘yeah’ ‘need’ ‘everybody’ ‘whoa’ ‘right’ ‘know’ ‘cause’ ‘alright’ ‘gotta’ 
• family/gospel (0.369): ‘little’ ‘lord’ ‘mama’ ‘work’ ‘help’ ‘daddy’ ‘hard’ ‘brother’ ‘sister’ ‘woman’ ‘home
• romantic (0.407): ‘hold’ ‘love’ ‘sweet’ ‘kiss’ ‘woman’ ‘hand’ ‘goodbye’ ‘touch’ ‘arm’ ‘darling’ ‘heart’ ‘true’
• communication/thinking (0.727): ‘know’ ‘tell’ ‘think’ ‘say’ ‘cause’ ‘things’ ‘talk’ ‘thing’ ‘mean’ ‘maybe’ 
• obscene (0.363): ‘like’ ‘fuck’ ‘cause’ ‘money’ ‘better’ ‘shit’ ‘know’ ‘about’ ‘bitch’ ‘party’ ‘play’ 
• sound (0.484): ‘home’ ‘sing’ ‘hear’ ‘blue’ ‘song’ ‘play’ ‘music’ ‘come’ ‘bring’ ‘sound’ ‘call’ • movement/places (0.543): ‘say’ ‘go’ ‘drink’ ‘road’ ‘ride’ ‘round’ ‘come’ ‘home’ ‘walk’ ‘city’ ‘look’ ‘drive’ ‘take’ 
• light/visual perceptions (0.462): ‘light’ ‘dance’ ‘turn’ ‘eye’ ‘night’ ‘burn’ ‘high’ ‘moon’ ‘come’ ‘dream’ ‘like’ ‘happiness’ 
• family/spiritual (0.454): ‘people’ ‘free’ ‘shake’ ‘young’ ‘mother’ ‘land’ ‘trouble’ ‘children’ ‘send’ ‘child’ ‘father’ 
• like/girls (0.547): ‘like’ ‘girl’ ‘look’ ‘alright’ ‘make’ ‘pretty’ ‘beautiful’ ‘girls’ ‘know’ ‘feel’ • sadness (0.510): ‘away’ ‘heart’ ‘fall’ ‘break’ ‘leave’ ‘tear’ ‘walk’ ‘lonely’ ‘feel’ ‘inside’ ‘wish’ ‘hurt’ ‘stay’ 
• feelings (0.581) ‘feel’ ‘go’ ‘good’ ‘somebody’ ‘miss’ ‘real’ ‘morning’ ‘strong’ ‘feeling’ ‘luck’ ‘wrong’ ‘thing’




We first load the dataset and the tidyverse package in preparation for further exploration.
```{r}
#Load the `tidyverse` package
pacman::p_load(tidyverse)

#Load the `music` dataset which is the csv file tcc_ceds_music
data(tcc_ceds_music)
```

We then apply head(), str(), and summary() to obtain an overview of the dataset.

```{r}
#Print the first 6 observations
head(tcc_ceds_music)
```


```{r}
# Counting unique values in the "track_name" column
unique_tracks <- unique(tcc_ceds_music$track_name)
num_unique_tracks <- length(unique_tracks)

# Print the number of unique tracks
print(num_unique_tracks)
```

Below is the code for a Boxplot with a color gradient

```{r}
install.packages("ggplot2")
install.packages("dplyr")
install.packages("magrittr")
library(ggplot2)
library(dplyr)
library(magrittr)

medians <- tcc_ceds_music %>%
  group_by(genre) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

tcc_ceds_music <- tcc_ceds_music %>%
  left_join(medians, by = "genre")

ggplot(tcc_ceds_music, aes(x = genre, y = danceability, fill = median_danceability)) +
  geom_boxplot() +
  scale_fill_gradient(low = "blue", high = "red") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Box Plot of Danceability Scores by Genre",
       x = "Genre",
       y = "Danceability",
       fill = "Median Danceability")
```
