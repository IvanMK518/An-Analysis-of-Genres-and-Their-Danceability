---
title: "R Notebook"
output: html_notebook
---

# Music Dataset : 1950 to 2019

## Introduction

Music is a core human experience, a fundamental part of our evolution. Music is an outlet for people to express and communicate with others. Some produce it and more consume and share it. It’s part of life that defines many people’s identities and helps to emphasize moods, whether uplifting or depressive, energetic or calming. The purpose in utilizing music data helps to understand how people’s behaviors over time are reflected and impact the music we hear. Though not necessarily simply a matter of change over time, we can discover what aspects are critical characteristics of each specific genre of music and its era of release through musical metadata. “There are several works involving musical lyrics and topic modeling, one in specific and very common used is the Latent Dirichlet Allocation (LDA) [Blei 2003]” (Misael, Luan, et al., 2020). LDA is a probabilistic model used to find latent semantic topics in collections of texts such as lyrics. We can cite the LyricsRadar [Sasaki 2014] which is a system based on latent topics of lyrics generated by LDA that enables a user to visualize the music topics interactively. LDA can also be applied to identify sentiments from songs through the lyrics just as audio analysis can [Sharma 2011, Dakshina 2013]. Using methods of creating data out of music using LDA provides a baseline for how we organize and characterize music for algorithms, such as one that you may see on Spotify. LDA is the start of how we can use statistical data to show how playlists are chosen by users, what features influence musical choices, and in some cases predict hits. Using audio data scrapped using Echo Nest® API integrated engine with spotipy Python’s package. The spotipy API permits the user to search for specific genres, artists, songs, release dates, etc. To obtain the lyrics we used the Lyrics Genius® API as base URL for requesting data based on the song title and artist name. Of the numerous song characteristics shown by the publicly available spotify based dataset, we chose to use a select few for in depth analysis in our quest to find what genres of music are most associated with dancing and danceability. Diving deeper, we wanted to understand what characteristics of a song made one more or less danceable. 

The first step taken to begin the process of  filtering of data from this public music data set, was to install the Rstudio packages and libraries shown below:

```{r}
# Set CRAN mirror and install necessary packages
options(repos = c(CRAN = "https://cran.rstudio.com"))

install.packages("evaluate")
install.packages("renv")
install.packages("tidyverse")
install.packages("ggplot2")
install.packages("dplyr")
install.packages("magrittr")

# Snapshot, restore, and hydrate renv environment
renv::snapshot()
renv::restore()
renv::hydrate()

# Load necessary libraries
library(tidyverse)
library(readr)
library(ggplot2)
library(dplyr)
library(magrittr)
```

The code snippet below is used in an R Markdown document to control the global options for all code chunks in the document. When echo = True, the code is shown with the corresponding output which makes it easier for authors and readers to see what the code is producing.
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

A preliminary step to filtering the data set is to test R and our music.csv file. This code snippet prints the first 6 observations, just to be sure we are working with the correct data set as R working directories may get crowded. 

```{r}
install.packages("tidyverse")
library(tidyverse)
```

```{r}
library(readr)
music <- read_csv("/Users/ivanmartinez-kay/QTM-302W/Data/music.csv") 
```
```{r}
head(music)
```

A few more preliminary steps in testing R and confirming the csv file are functioning are to play around with the csv file. 

```{r}
str(music)

print(names(music))

summary(music)
```

First, Using the "str" function helps us see the structure of the data set in that we have #31 columns with quantitative (numerical) and qualitative data such as a certain characteristic. The "print(names(music))" and "summary" functions just provide an organized view of the column titles and the sample characteristics (quartiles, median, and mode) of each column respectively. 

A data set with so much qualitative data may be initially difficult to process, but if the qualitative data is assigned numerical values, analyzing the data can be done with concrete methodology that can be replicated. In order to quantify and assign values to a seemingly qualitative characteristic such as lyrical theme, the creators of the public data set used:

EQUATION 1: 1. similarity = cos(Θ) = ϕi · ϕj /(ϕi ϕj)

This equation would compute a number between 0 and 1 that would effectively act as a piece of raw data representing a lyrical characteristic or theme of a song. According to the creators of the data set:

"We evaluated the cosine distance, Eqn. (1), of words per topic using a 100-dimension GloVe word embedding [Pennington 2014] vector representation. To visualize the best number of words to represent each topic, we select every word proportion per topic k represented by ϕk distribution by the most frequent term in that topic to the rarest term.For each topic, we have chosen the first 10 words of ϕK and computed the cosine distance for every pair of their embedding vectors and stored the mean result in a variable gk,10. We repeated the same methodology for the first 11 to 25 words of ϕk in each topic k calculating the mean cosine distance of the word embeddings. Based on those criteria, we select an appropriate number of words to represent each topic that maximizes the mean cosine similarity. Each topic received a label based on the main subject of the selected words. A stop word list was used to exclude common English words. The stop word list used is built-in Scikit-learn." (Misael, Luan, et al., 2020)

In our quest to find which songs and corresponding genres from the dataset were most danceable we considered a few song characteristics:

  * Acousticness: Presence of acoustic instruments
  * Danceability: how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat, strength, and overall regularity
  * Loudness: The average loudness in decibels (dB) across the entire track
  * Instrumentalness: A high value describes whether a track contains fewer vocals
  * Valence: High (low) values means that the track is more happy, euphoric (sad, angry)
  * Energy: Measures intensity and activity of music. Energetic tracks will be fast, loud and noisy
Each characteristic as mentioned would be assigned a value between 0-1 based on equation 1 (shown above).

## Analysis

The code snippet just below is selecting specific columns from our initial music.csv data set and assigning them to a new data frame called "data" that includes only these selected columns (consisting of data necessary for this coding and analysis).

```{r}
data <- music[ ,c("genre", "loudness", 
"danceability", "feelings", "instrumentalness", "acousticness",
"valence", "energy")]
```

In this small code chunk below, the median danceability by genre is found from the data set.

```{r}
median_danceability_by_genre <- data %>%
  group_by(genre) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

print(median_danceability_by_genre)
```

The first line is creating a data frame called "median_danceability_by_genre" from the "data" data frame and this pipe operator (%>%) allows the author to chain multiple functions together in a clear and readable way. The data frame data is passed to the next function in the chain. Then using the "group_by" function the data frame is grouped by the genre column. This means that subsequent operations will be performed separately for each genre. The 2nd to last line creates a new data frame containing summary statistics using "summmarise." In this case, it will create one row per genre. "median(danceability, na.rm = TRUE)" will calculates the median of the danceability column for each genre group from the data set. The na.rm = TRUE argument ensures that any NA (missing) values are removed before calculating the median. Finally, the new data frame is printed, showing a 2 column chart with 7 rows. Each row should show a genre in one column with a corresponding median danceability score in the other.

```{r}
# Create a bar chart of median danceability by genre
ggplot(median_danceability_by_genre, aes(x = genre, y = median_danceability)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Genre",
       x = "Genre",
       y = "Median Danceability")
```

From the above visualization, we notice that hiphop genre has the highest media danceability, closely followed by reggae, which similar to what people would generally assume. The genre with the lowest danceability is rock, with the second lowest genre being blues. As blues is a rather slow genre, we are not surprised to see it ranked so low. However, rock, being such a rhythmic genre but ranking as the least danceable genre, is quite surprising to us. 

For the next part, we mainly want to visualize media danceability by different values of feeling. To do so, we first need to calculate the media danceability by feeling, which is accomplished by the first three lines of the following code, where the first line initiates a pipeline for the dataset, the second line grouping the data by the feeling columns, and the thid line calculating the median of the danceability column for each group of feelings. As a sidenote, the na.rm = TRUE argument ensures that any NA (missing) values in the danceability column are ignored during the median calculation. Then, the fourth line prints the median_danceability_by_feeling dataframe, which contains the median danceability values for each feeling.

```{r}
library(dplyr)

# Calculate median danceability by feeling
median_danceability_by_feeling <- data %>%
  group_by(feelings) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

# Print the median danceability by feeling
print(median_danceability_by_feeling)
```

In the data, it is obviously very difficult to grasp what value or what range of values of feeling has the highest median danceability, despite that we have successfully calculated the data. To give ourselves a clearer idea of the data at hand, we turn to the next step - visualization.

The following code could be understood by grouping them into three different sets, as separated by the comments below. As the feeling is a set of continuous variables, we cannot simply separate them into different columns. To this end, we have the first set of code setting the number of bins to 20 for condensing the data of musical feelings into 20 different columns. The, we start a pipeline for a new dataframe for visualization and create a new column 'feeling_bin' to contain the newly separated 20 columns of values for feelings.

The second set of code repeats the calculation of the median danceability values for each feeling. Then the third set of code turn the calculation into visualization, specifically, a bar chart. The first line initializes a ggplot object with the new variable feeling_bin on the x-axis and median_danceability on the y-axis. The second line creates a bar chart where the height of the bars represents the median danceability values with the color blue. The third line adjusts the x-axis text to be displayed vertically to improve readability. Finally, the fourth line adds a title and labels to the x and y axes.

```{r}
library(readr)
library(ggplot2)

# Bins to condense data of musical feel 
num_bins <- 20 
data <- data %>%
  mutate(feeling_bin = cut(feelings, breaks = num_bins))

# Calculate median danceability by feel of music
median_danceability_by_feeling_bin <- data %>%
  group_by(feeling_bin) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

# Bar chart of median danceability by feel
ggplot(median_danceability_by_feeling_bin, aes(x = feeling_bin, y = median_danceability)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Feel",
       x = "Feel",
       y = "Median Danceability")
```
```{r}
library(dplyr)
library(ggplot2)

# Assuming your data frame is named 'data'

# Group data by genre, calculate median feeling
median_feeling_by_genre <- data %>%
  group_by(genre) %>%
  summarise(median_feeling = median(feelings, na.rm = TRUE)) %>%
  ungroup()

# Create a bar chart to visualize median feeling by genre
ggplot(median_feeling_by_genre, aes(x = genre, y = median_feeling)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Feeling by Genre",
       x = "Genre",
       y = "Median Feeling")
```

The bar charts in fact have roughly equal heights, with minor variability not exceeding 0.1, except one outlier for the feelings between [0.671 to 0.719]. The sudden drop in median danceability can be due to a lot of factors. Our guess would be it might be a coincidence - if we devide up the feelings differently, for example, into 15 columns or into 25 columns instead of 20 columns, such gap in the median danceability might not be the case at all. 

Moving on, we again want to measure the median danceability by instrumentalness. The underlying logic for calculation remains the same, where the first line initiates a pipeline for the dataset, the second line grouping the data by the instrumentalness columns, and the third line calculating the median of the danceability column for each group of instrumentalness, the fourth line printing the final dataset, which contains the median danceability values for each instrumentalness. 

```{r}
library(dplyr)

# Calculate median danceability by instrumentalness
median_danceability_by_instrumentalness <- data %>%
  group_by(instrumentalness) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

# Print the median danceability by instrumentalness
print(median_danceability_by_instrumentalness)
```

Moving onto the visualization aspect, we again use similar codes, where we condense the coutinuous variable instrumentalness into 20 different columns, calculate the median danceability by instrumentalness, and then visualizing the dataset using bar chart.

```{r}
# Create bins for the instrumentalness variable
num_bins <- 20 
data <- data %>%
  mutate(instrumentalness_bin = cut(instrumentalness, breaks = num_bins))

# Calculate median danceability by instrumentalness bin
median_danceability_by_instrumentalness <- data %>%
  group_by(instrumentalness_bin) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

# Bar chart of median danceability by instrumentality
ggplot(median_danceability_by_instrumentalness, aes(x = instrumentalness_bin, y = median_danceability)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Instrumentality",
       x = "Instrumentality",
       y = "Median Danceability")
```

```{r}
library(dplyr)
library(ggplot2)

# Assuming your data frame is named 'data'

# Group data by genre, calculate median feeling
median_instrumentalness_by_genre <- data %>%
  group_by(genre) %>%
  summarise(median_instrumentalness = median(instrumentalness, na.rm = TRUE)) %>%
  ungroup()

# Create a bar chart to visualize median feeling by genre
ggplot(median_instrumentalness_by_genre, aes(x = genre, y = median_instrumentalness)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Instrumentality by Genre",
       x = "Genre",
       y = "Median Instrumentality")
```

The resulting chart, again doesn't show a clear trend in terms of change of median danceability between differnet levels of instrumentality. However, we would argue that roughly speaking, lower instrumentality is correlated with higher median danceability, as the bar chart's height shows a slight decreasing trend in median danceability as instrumentality increases. Recalling that a higher score in instrumentalness indicates a lower level of vocal a track contains, we can perhaps reach the conclusion that the more vocals a track contains, the more danceable the track is. 

```{r}
# Bins for condensing data inthe feelings and instrumentalness variables
num_bins_feelings <- 2
num_bins_instrumentalness <- 2

data <- data %>%
  mutate(feeling_bin = cut(feelings, breaks = num_bins_feelings),
         instrumentalness_bin = cut(instrumentalness, 
         breaks = num_bins_instrumentalness))

# Calculate median danceability by genre, feelings bin, and instrumentalness bin
median_danceability_by_genre_feeling_instrumentalness <- data %>%
  group_by(genre, feeling_bin, instrumentalness_bin) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

# Create a bar chart of median danceability by genre, feelings bin, and instrumentalness bin
ggplot(median_danceability_by_genre_feeling_instrumentalness, aes(x = interaction(feeling_bin, 
instrumentalness_bin), y = median_danceability, fill = genre)) +
  geom_bar(stat = "identity", position = "dodge") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Genre, Feeling, and Instrumentalness",
       x = "Feeling and Instrumentalness",
       y = "Median Danceability",
       fill = "Genre")
```

This graph synthesizes the information of the three previous bar charts. It once again demonstrates that even on a basis of feel and instumentality, reggae and hip hop are among the most consistently danceable subcategories of music.

```{r}
library(dplyr)

# Calculate median danceability by decibel level
median_danceability_by_loudness <- data %>%
  group_by(loudness) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

print(median_danceability_by_loudness)
```

Finally, we want to explore the correlation between median danceability with decibel level for each track. To see a clearer trend, we do not want to use bin to condense the variables into a few numbers of columns. This first line initializes a ggplot object using the median_danceability_by_loudness dataframe, setting loudness as the x-axis variable and median_danceability as the y-axis variable. The second line adds a bar chart layer to the plot. As a side note, the 'stat = "identity"' argument specifies that the heights of the bars should represent the actual values of median_danceability rather than counts, whereas the fill = "blue" argument colors the bars blue. The third line adjusts the x-axis text to be displayed vertically, improving readability, especially given that our original x-axis labels are very long. Finally ,the fourth line adds a title and labels to the x and y axes: The title of the chart is "Median Danceability by Loudness," the x-axis is labeled "Loudness (Db)," and the y-axis is labeled "Median Danceability."

The resulting visualization has two peaks for median danceability, one around 0.5 Db and the other around 0.75 Db. There is also a very clear decrease in median danceability around 0.67 Db. We cannot be certain of the reason behind such trend yet, but we have taken notes of the trend and would be interested in finding out more about the reason behind in the next project.

```{r}
library(readr)
library(ggplot2)

# Bar chart of median danceability by decibel level
ggplot(median_danceability_by_loudness, aes(x = loudness, y = median_danceability)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Loudness",
       x = "Loudness (Db)",
       y = "Median Danceability")
```

```{r}
library(readr)
library(ggplot2)

# Line graph of median danceability by decibel level
ggplot(median_danceability_by_loudness, aes(x = loudness, y = median_danceability)) +
  geom_line(color = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Loudness",
       x = "Loudness (Db)",
       y = "Median Danceability")
```

```{r}
library(dplyr)

# Calculate median danceability by decibel level
median_danceability_by_acousticness <- data %>%
  group_by(acousticness) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

print(median_danceability_by_acousticness)
```

```{r}
library(readr)
library(ggplot2)

# Line graph of median danceability by decibel level
ggplot(median_danceability_by_acousticness, aes(x = acousticness, y = median_danceability)) +
  geom_line(color = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Danceability by Acousticness",
       x = "Acousticness",
       y = "Median Danceability")
```

```{r}
library(dplyr)

# Calculate median danceability by instrumentalness
median_danceability_by_instrumentalness <- data %>%
  group_by(instrumentalness) %>%
  summarise(median_danceability = median(danceability, na.rm = TRUE))

# Print the median danceability by instrumentalness
print(median_danceability_by_instrumentalness)
```

Moving onto the visualization aspect, we again use similar codes, where we condense the coutinuous variable instrumentalness into 20 different columns, calculate the median danceability by instrumentalness, and then visualizing the dataset using bar chart.

```{r}
# Create bins for the instrumentalness variable
num_bins <- 20 
data <- data %>%
  mutate(instrumentalness_bin = cut(instrumentalness, breaks = num_bins))

# Calculate median danceability by instrumentalness bin
median_acousticness_by_instrumentalness <- data %>%
  group_by(instrumentalness_bin) %>%
  summarise(median_acousticness = median(acousticness, na.rm = TRUE))

# Bar chart of median danceability by instrumentality
ggplot(median_acousticness_by_instrumentalness, aes(x = instrumentalness_bin, y = median_acousticness)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Acousticness by Instrumentality",
       x = "Instrumentality",
       y = "Median Acousticness")
```

```{r}
library(dplyr)
library(ggplot2)

# Assuming your data frame is named 'data'

# Group data by genre, calculate median feeling
median_acousticness_by_genre <- data %>%
  group_by(genre) %>%
  summarise(median_acousticness = median(acousticness, na.rm = TRUE)) %>%
  ungroup()

# Create a bar chart to visualize median feeling by genre
ggplot(median_acousticness_by_genre, aes(x = genre, y = median_acousticness)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Acousticness by Genre",
       x = "Genre",
       y = "Median Acousticness")
```

```{r}
library(dplyr)
library(ggplot2)

# Assuming your data frame is named 'data'

# Group data by genre, calculate median feeling
median_acousticness_by_genre <- data %>%
  group_by(genre) %>%
  summarise(median_acousticness = median(acousticness, na.rm = TRUE)) %>%
  ungroup()

# Create a bar chart to visualize median feeling by genre
ggplot(median_acousticness_by_genre, aes(x = genre, y = median_acousticness)) +
  geom_bar(stat = "identity", fill = "blue") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Acousticness by Genre",
       x = "Genre",
       y = "Median Acousticness")
```

```{r}
library(dplyr)
library(ggplot2)

# Assuming your data frame is named 'data'

# Group data by genre, calculate median feeling
median_valence_by_genre <- data %>%
  group_by(genre) %>%
  summarise(median_valence = median(valence, na.rm = TRUE)) %>%
  ungroup()

# Create a bar chart to visualize median feeling by genre
ggplot(median_valence_by_genre, aes(x = genre, y = median_valence)) +
  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = 1) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Valence by Genre",
       x = "Genre",
       y = "Median Valence")
```
```{r}
# Group data by genre, calculate median feeling
median_energy_by_genre <- data %>%
  group_by(genre) %>%
  summarise(median_energy = median(energy, na.rm = TRUE)) %>%
  ungroup()

# Create a bar chart to visualize median feeling by genre
ggplot(median_valence_by_genre, aes(x = genre, y = median_energy)) +
  geom_dotplot(binaxis = 'y', stackdir = 'center', dotsize = 1) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(title = "Median Energy by Genre",
       x = "Genre",
       y = "Median Energy")
```


## Conclusion

Overall speaking, in this report we focus on danceability of a given track, aiming to find out the relationship between danceability with four other variables: genre, feelings, instrumentalness, and loudness. Although feelings doesn't seem to have any noticeable relationship with danceability, we did find out that the other three variables having some potentially interesting relationships with danceability. Specifically, hip hop has the highest median danceability while rock has the lowest; lower instrumentality usually indicate higher median danceability; track with 0.67 Db of loudness usually has the lowest median danceability, while track with 0.5 Db and 0.75 Db of loudness have the highest. 

We would like to further explore the relationship between danceability with other variables and the reasons behind in our upcoming research project. We hope that this could serve as an replicable analysis model for people finding danceable music - the next time they are in the mood for dancing. Danceability is an exciting song characteristic, but it is by no means the only one that can be analyzed. Additionally, "data preparation" and analyses similar to this code notebook breakdown function as a baseline for song recommendation algorthims on streaming platforms. Taking playlist data and finding matches in songs (and artists) with similar characeristics is a large part of manufacturing popularity surrounding new music. 


Works cited:

Shahane, S. (2019). Music Dataset: 1950 to 2019 [Data set]. Kaggle. https://www.kaggle.com/datasets/saurabhshahane/music-dataset-1950-to-2019/data

Misael, L., Forster, C., Fontelles, E., Sampaio, V., & França, M. (2020, October). Temporal analysis and visualisation of music. In Anais do XVII Encontro Nacional de Inteligência Artificial e Computacional (pp. 507-518). SBC.
